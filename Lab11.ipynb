{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/tge2bD2Dt0bJQWBp4+f7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glorivaas/Machine_Learning25/blob/main/Lab11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HOMEWORK 11**\n",
        "##**Understanding Deconvolution in Autoencoders**"
      ],
      "metadata": {
        "id": "fuywdbk6LC87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Theoretical part**\n",
        "\n",
        "1. **What is a transposed convolution?**\n",
        "\n",
        " A transposed convolution (also called deconvolution) is a layer that reverses the spatial effects of a standard convolution.\n",
        " Its goal is to upsample a smaller input feature map into a larger output. It is often used in the field of autoencoders (in the decoder), GANs (to generate images), U-Net (to recover spatial resolution).\n",
        "  \n",
        "  We can think of it as answering the following question: \"How do we reconstruct a larger image that might have been downsampled by a convolution before?\"\n",
        "\n",
        "\n",
        "2. **How does it differ from a regular convolution?**\n",
        "\n",
        " The goal of a regular convolution is to downsample, or reduce the size of the input (which are larger elements than the output), whereas transposed convolution focuses on upsampling, or increasing the size of the inputs. Moreover, their usses differ, being mainly encoding (feaure extraction) for regular convolution and decoding (reconstruction, generation) for transposed one.\n",
        " Their way of operating is also different: in regular convolution, the kernel compresses information into smaller areas. In transposed convolution, we apply the kernel in a way that distributes the input values over a larger space, effectively expanding it.\n",
        "\n",
        "\n",
        "3. **How does it upsample feature maps?**\n",
        "\n",
        " The main steps for increasing the spatial dimensions of an input tensor are the following:\n",
        "  1. Insert zeros between elements (depending on stride).\n",
        "\n",
        "  2. Slide a kernel over this expanded input.\n",
        "\n",
        "  3. Multiply and sum values just like in convolution.\n",
        "\n",
        "  4. Overlap adds up: multiple kernel applications can contribute to the same output position.\n",
        "\n",
        "  5. The result is an upsampled output grid with learned spatial structure.\n",
        "\n",
        "  Maybe it is clearer if we see an example for this.\n",
        "\n",
        "    - Input feature map: a 2D grid (size 2×2)\n",
        "\n",
        "    - Kernel: size 3×3\n",
        "\n",
        "    - Stride: 2\n",
        "\n",
        "    - Padding: 0\n",
        "\n",
        "  We'll show what happens when we apply a transposed convolution.\n",
        "\n",
        "- **Step 1**: Insert Zeros. This depends on the stride.\n",
        "\n",
        "  If stride = 2, insert (stride - 1) = 1 zero between each row and column.\n",
        "\n",
        "  For example, for a 2×2 input: <br>\n",
        "  Input: <br>\n",
        "  [1 2] <br>\n",
        "  [3 4]\n",
        "\n",
        "  Expanded: <br>\n",
        "  [1 0 2] <br>\n",
        "  [0 0 0]<br>\n",
        "  [3 0 4] <br>\n",
        "  Now we have a 3×3 grid, where the non-zero values are the original inputs, and the rest are zeros.\n",
        "\n",
        "  Note: With larger strides, more zeros are inserted.\n",
        "\n",
        "- **Step 2**: Slide the kernel over the expanded grid.\n",
        "\n",
        "  Let’s say the kernel is: <br>\n",
        "  [a b c] <br>\n",
        "  [d e f] <br>\n",
        "  [g h i] <br>\n",
        " At each position, we perform the usual element-wise multiplication and summation between the kernel and the overlapping region in the expanded grid.\n",
        "\n",
        " For instance, the top-left region of the expanded grid might be multiplied with the kernel, and the sum is written into the output at a specific location.\n",
        "\n",
        " This step is why the operation is called “transposed” — mathematically, it's the gradient of a convolution, but operationally it \"spreads out\" the input.\n",
        "\n",
        "- **Step 3**: sum overlapping contributions. <br>\n",
        " Because we are sliding the kernel over a grid that has inserted zeros and a stride of 1, some output positions will receive multiple contributions from different overlapping kernel applications.\n",
        "\n",
        "  In those cases, we sum the values contributed by each overlapping region.\n",
        "\n",
        "- **Final result** <br>\n",
        "  The result is an output grid larger than the input. For example:\n",
        "\n",
        "  If you start with a 2×2 input, a 3×3 kernel, and stride = 2, you may end up with a 5×5 output, depending on the padding and dilation.\n",
        "\n",
        "\n",
        "  <br>\n",
        "\n",
        "  **Note**: formula to compute output size\n",
        "\n",
        "  Output size = (Input size - 1) * stride - 2 * padding + kernel size\n",
        "\n",
        "  Example:\n",
        "\n",
        "  Input = 2\n",
        "\n",
        "  Stride = 2\n",
        "\n",
        "  Padding = 0\n",
        "\n",
        "  Kernel size = 3\n",
        "\n",
        "  Then:\n",
        "\n",
        "  (2 - 1) * 2 - 0 + 3 = 5\n",
        "\n",
        "4. **What are stride, padding, and kernel size, and how do they influence the result in a transposed convolution?**\n",
        "\n",
        "  1. **Stride**:\n",
        "    - is the step size with which the kernel is moved over the input.\n",
        "    - controls how far apart the output patches (generated from each input value) are placed.\n",
        "\n",
        "    - in transposed convolution, a larger stride increases the spacing between the output contributions, thereby increasing the output size.\n",
        "\n",
        "    - Mathematically, the output size increases with stride, often approximately multiplied by the stride value.\n",
        "\n",
        " 2. **Padding**:\n",
        "    - in transposed convolution refers to the amount of cropping or shifting done after expanding the input via stride and kernel.\n",
        "\n",
        "    - in contrast to regular convolution (where padding adds zeros around the input), in transposed convolution it affects how the output is trimmed.\n",
        "\n",
        "    - proper padding ensures that the transposed convolution reverses the spatial transformations done by the original convolution (in encoder-decoder architectures like autoencoders or U-Nets).\n",
        "\n",
        "  3. **Kernel size**:\n",
        "    - the kernel size refers to the height and width of the filter (EX. 3×3).\n",
        "\n",
        "    - this kernel is applied to each input element, generating a region of influence in the output.\n",
        "\n",
        "    - larger kernels increase the area affected by each input value, which leads to greater spread in the output.\n",
        "\n",
        "  They are all related to the output size by the previous formula: <br>\n",
        "  Output size = (Input size - 1) * stride - 2 * padding + kernel size"
      ],
      "metadata": {
        "id": "PKmMc0cTLYpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Manual diagram**"
      ],
      "metadata": {
        "id": "5yVYUkIC9Np3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![My Diagram](https://github.com/glorivaas/Machine_Learning25/blob/main/CamScanner%2006-02-2025%2011.18.pdf)"
      ],
      "metadata": {
        "id": "TL5Qrviq8sTv"
      }
    }
  ]
}