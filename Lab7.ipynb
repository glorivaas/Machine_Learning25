{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBQSqfbp2GyUuh0gpSQsbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glorivaas/Machine_Learning25/blob/main/Lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 7 - Gradient Boosting\n",
        "\n",
        "### Author: Gloria Rivas"
      ],
      "metadata": {
        "id": "_QrK8xwY-rFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Derivation and Analysis**\n",
        "\n",
        "  **Scenario A:**\n",
        "  - Derive explicitly the optimal $\\lambda$ for fitting from scratch, i.e., solve\n",
        "    \n",
        "    $$\n",
        "    \\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, \\lambda)\n",
        "    $$\n",
        "  <br>Where the loss is:\n",
        "  $$\n",
        "L(y_i, \\lambda) = -y_i \\log(\\sigma(\\lambda)) - (1 - y_i) \\log(1 - \\sigma(\\lambda))\n",
        "$$\n",
        "\n",
        "with the sigmoid function:\n",
        "\n",
        "$$\n",
        "\\sigma(\\lambda) = \\frac{1}{1 + e^{-\\lambda}}\n",
        "$$\n",
        "\n",
        "Let:\n",
        "- $ m $: number of samples where $ y_i = 1 $\n",
        "- $ k $: number of samples where $ y_i = 0 $\n",
        "- $ n = m + k $\n",
        "\n",
        "Then the total loss becomes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\lambda) = -m \\log(\\sigma(\\lambda)) - k \\log(1 - \\sigma(\\lambda))\n",
        "$$\n",
        "\n",
        "Take the derivative with respect to $ \\lambda $:\n",
        "\n",
        "$$\n",
        "\\frac{d\\mathcal{L}}{d\\lambda} = \\sigma(\\lambda)(1 - \\sigma(\\lambda)) \\left( -\\frac{m}{\\sigma(\\lambda)} + \\frac{k}{1 - \\sigma(\\lambda)} \\right)\n",
        "$$\n",
        "\n",
        "Simplify:\n",
        "\n",
        "$$\n",
        "= -m (1 - \\sigma(\\lambda)) + k \\sigma(\\lambda)\n",
        "$$\n",
        "\n",
        "Set derivative to zero:\n",
        "\n",
        "$$\n",
        "-m + m \\sigma(\\lambda) + k \\sigma(\\lambda) = 0\n",
        "\\Rightarrow \\sigma(\\lambda)(m + k) = m\n",
        "\\Rightarrow \\sigma(\\lambda) = \\frac{m}{m + k} = \\frac{m}{n}\n",
        "$$\n",
        "\n",
        "Solve for $ \\lambda $:\n",
        "\n",
        "$$\n",
        "\\sigma(\\lambda) = \\frac{1}{1 + e^{-\\lambda}} = \\frac{m}{n}\n",
        "\\Rightarrow \\lambda^* = \\log\\left( \\frac{m}{k} \\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "Final Result\n",
        "\n",
        "$$\n",
        "\\boxed{\\lambda^* = \\log\\left( \\frac{m}{k} \\right)}\n",
        "$$\n",
        "\n",
        "---\n",
        "**Interpretation**\n",
        "\n",
        "- The optimal constant $$ \\lambda^* $$ reflects the log-odds of the class distribution.\n",
        "- If $ m = k $, then $ \\lambda^* = 0 $ (neutral prediction).\n",
        "- If $ m > k $, then $ \\lambda^* > 0 $ : model favors class 1.\n",
        "- If $ m < k $, then $ \\lambda^* < 0 $ : model favors class 0.\n",
        "\n",
        "### Scenario B\n",
        "\n",
        "Now assume that we already have predictions $ f_i = f_{m-1}(x_i) $, and we want to add a constant shift $ \\lambda $:\n",
        "\n",
        "$$\n",
        "\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^n L(y_i, f_i + \\lambda)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "L(y_i, f_i + \\lambda) = -y_i \\log(\\sigma(f_i + \\lambda)) - (1 - y_i) \\log(1 - \\sigma(f_i + \\lambda))\n",
        "$$\n",
        "\n",
        "Let $ s_i = \\sigma(f_i + \\lambda) $, then:\n",
        "\n",
        "$$\n",
        "\\frac{d\\mathcal{L}}{d\\lambda} = \\sum_{i=1}^n \\left( s_i - y_i \\right)\n",
        "= \\sum_{i=1}^n \\left( \\sigma(f_i + \\lambda) - y_i \\right)\n",
        "$$\n",
        "\n",
        "Set derivative to zero:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\sum_{i=1}^{n} \\left[ \\sigma(f_i + \\lambda) - y_i \\right] = 0\n",
        "}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "- In Scenario A, all predictions are constant. The optimization is simple and convex.\n",
        "- In Scenario B:\n",
        "  - Each $ f_i $ is different.\n",
        "  - The sigmoid is applied to $ f_i + \\lambda $, making the loss **nonlinear** and **non-separable**.\n",
        "  - The result is a **complex loss landscape** with **no closed-form** for $ \\lambda $.\n",
        "  - Must be solved **numerically** (e.g., gradient descent).\n",
        "\n",
        "The difficulty arises from the interaction between the **non-linearity of the sigmoid function** and the **variability in the previous predictions**."
      ],
      "metadata": {
        "id": "IX76yuPG_QgX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8f0Sp8M-iQ5"
      },
      "outputs": [],
      "source": []
    }
  ]
}